Hello everybody!
My name is Bernhard Fritz and I'd like to present you my bachelor project about procedural generation of mountain ranges based on geology.

Let me start with a motivational video.
What you can see here, is an implementation of a basic tectonic plate movement simulation.
Whenever two plates collide, there will be a change in height.
I will tell you later in this presentation how this works in detail.

Now that you are all motivated, let me give you a brief overview.
At first I will tell you what this thesis was all about and what challenges had to be encountered.
Then I will explain the data structures that were absolutely essential for the algorithms and simulations I am going to present to you today.
There are two categories of terrain generation algorithms.
Geology and non-geology-based algorithms.
Geology-based algorithms often depend on their non-geology-based counterparts, that is why both types will be discussed.
At the end I will show you some visualization techniques that were used for this bachelor project.

So what was this bachelor project all about.
I started by implementing and testing several existing terrain generation algorithms.
Doing that I learnt about the essential features a procedural terrain generation algorthm must provide and then implemented my own procedural terrain generation algorithm.

In order to build geology-based algorithms and simulations, it is necessary to have a basic understanding of geology.

The tectonic plate simulation is a simplified model of natural phenomena occuring on earth and was used to generate plate collision data.

Besides terrain generation, it is also important to understand how nature degradates landscape and how this behavior can be implemented in code.
I therefore studied two different kinds of erosion and implemented them both.

So what what was challenging about this project?
There are a lot of related papers available.
Some of them are very well documented.
Others aren't.
At one particular case I had to combine information from different sources in order to reproduce the showcased results.

When programming computer graphics algorithms and simulations, things can get out of hand quite quickly in terms of memory and processing requirements.
As a result it was often necessary to review the code and optimize it wherever possible.

Although it was difficult at times, I can say with confidence that all final versions of the algorithms and simulations that I will present today are in fact real-time capable.

Alright now let's get started with the essential data structures.
For terrain generation algorithms and simulations it is quite common to use heightmaps to encode elevation data.
The reason for this is simple, heightmaps are fast and they are good enough for most applications.
On heightmaps, height is well defined for any arbitrary position.
This also means there cannot be any landscape overhangs.
Alternatives to heightmaps would be voxel or marching cube-based models.
These models would be able to handle overhanging landscapes but they are far less efficient and it would be much more difficult to use them for procedural terrain generation.
Heightmaps consist of tiles.
These tiles consist of triangles.
And these triangles again consist of vertices.
On the bottom left we can see a 3x3 heightmap.
3x3 because it consists of 3 horizontal and 3 vertical tiles.
Heightmaps can have any arbitrary dimension.
Usually they have square dimensions though.
A 3x3 heightmap consists of 4x4 vertices.
These vertices are saved in an indexed datastructure to save space.
This means a vertex can be used by several triangles.
In case of heightmaps, a maximum of 6 triangles can contain the same vertex.
Let's say I change this vertex in height, this would affect all 6 triangles since they contain the same vertex.
When defining an indexed datastructure it is important to define the triangle vertices in a consistent order.
In my case, I decided to define them in counter-clock-wise order.
There are different ways how to arrange the triangles.
I sticked to what most papers where using and this is what I ended up with.

Since heightmaps only have one height value at any coordinate tuple (x,z) it is possible to simply save them as an 8-bit grayscale image.
With 1 byte per pixel there are 256 different height values for each vertex.
On the left you can see such a grayscale image.
The dark regions represent low elevation and light regions high elevation respectively.
On the right you can see a 3D representation of the same heightmap.
Since heightmaps can simply be saved as an image, it is very easy to import them into other applications.

This slide is just about categorization.
We will talk about two types of non-geology-based terrain generation algorithms:
Random, hardly controllable terrain generation algorithms and parametrically controllable terrain generation algorithms.
The diamond-square and the fault algorithm are very limited in configurability.
The repeated magnification and probing algorithm on the other hand can easily be customized by several parameters.

Let me start with the random, hardly controllable terrain generation algorithms.
There are two candidates that I want to talk to you about:
The diamond-square and the fault algorithm.

Before explaining diamond-square algorithm, I'd like to show you a closely related algorithm called midpoint-displacement.
So the idea is that you recursively calculate the midpoints and displace them vertically by a random amount.

Now the diamond-square algorithm is similar to midpoint-displacement.
Diamond-square just adds another dimension.
Here the midpoint is basically the center of the points in concern.
And the displacement is based on the average height of the points in concern plus a random displacement.
Here we can see diamond-square algorithm applied to a 4x4 heightmap.
So it starts by initialization of the corner vertices.
Orange vertices are being altered in height.
Blue vertices are affecting the height calculation of the orange vertices.
And the gray arrows are just additional indicators, which vertices affect which.
A diamond step takes the average of each square and sets the midpoint of each square to be the average of its four corner vertices plus a random displacement.
A square step takes the average of each diamond and sets the midpoint of each diamond to be the average of its corner vertices plus a random displacement.
By recursively calling these steps in an alternating fashion a fairly random looking landscape can be generated.
I prepared a little demonstration that should showcase these steps much more clearly.
These screenshots showcase the landscape that can be generated using this approach.
Keep in mind, this is a random, hardly controllable algorithm and it is not possible to control the location or spread of a mountain.
The only thing that can be configured is the roughness of the terrain.
By increasing the random displacement amount, it is possible to generate much rougher terrain for example.

Besides diamond-square algorithm there is also another random, hardly controllable terrain generation algorithm called fault algorithm.
The goal is to continuosly split the height map in two regions and then raise one and lower the other.
So how can we split the heightmap in two regions?
This can simply be done by vector math.
The vector product alone is actually all that we need.
Let's say we want to know if point p is in one or the other region defined by the intersection line from a to b.
Then we first calculate two vectors that have the same origin, in this case p-a and b-a with origin a.
The result of the vector product is a vector that points in a direction perpendicular to both input vectors.
The direction of the resulting vector can be used as an indicator wheter point p is in one or the other region.
A simple way to remember which direction the resulting vector is pointing to is by using the right hand rule.
By pointing with your index finger along the first and your middle finger along the second vector, then your thumb will point in the direction of the result vector.
In the program, the sign of the z-coordinate will indicate whether p is in one or the other region.
This is due to the fact that vector p-a as well as b-a are on the xy-plane.
Therefore the result vector must point directly along the positive or negative z-axis.
Dark green represents low and light green high elevation.
If you execute this algorithm for several 100 iterations, you can imagine that this might result in a fairly random looking landscape.
The terrain produced by this algorithm is pretty rough looking.
Usually after using fault algorithm it is recommended to erode the terrain using a thermal erosion algorithm as seen here.
I will later show you how this algorithm works in detail.
But now let's continue with parametrically controllable terrain generation algorithms.
So the algorithms we talked before did not offer any configurability.
It was not possible to control the location or shape of a mountain.
I will now present to you an algorithm that is capable to do both of that.
It is called repeated magnification and probing algorithm and was originally published by Kamal and Uddin.
The graph-based version of RMP is closely related to the original version of Kamal and Uddin.
The general idea is that you have these layers of polygon grids and you raise some of these polygons per iteration. The amount of connected polygons raised per iteration has influence on the spread of the mountain. The choice which polygons should be raised and which shouldn't influences the location of the resulting mountain.
Now how can we generate these polygon grids?
At first we generate four lines representing the borders of the heightmap.
Then we place an arbitrary amount of random cutting lines.
It is important to remember the start as well as end point of these lines.
This information will later be used to create the graph edges.
In the next step all lines will be iterated through and the intersections will be determined.
These intersections will represent the vertices of the graph.
Iterating through the vertices of the current line it is possible to add graph edges between the vertices along the line.
If a vertex already is a part of the graph it will not be added again.
Since two intersecting lines share the same intersection point it is obvious that the resulting vertex only has to be added to the graph once.
Once all the vertices and edges have been added to the graph we can continue with the next step.
Now what we care about are the polygons contained within the graph.
Finding the polygons means we have to determine all minimal cycles of the graph.
This is actually harder than it sounds.
I will introduce you to a simple but inefficient algorithm that allows you to find all minimal cycles.
The first step would be to find all cycles.
And then in a second step filter out duplicates or cycles that contain other cycles.
This can be done by using a depth-first-search.
After the depth first search has been performed, it is possible to do quick checks to see if there are any connections between the current vertex and the other vertices on the DFS path.
If there is a connection the resulting cycle will be added to the unfiltered cycle list.
Once the full path has been expended, the whole procedure will start over again, using a different vertex as starting point of the DFS.
Once this procedure has been performed using each and every vertex as starting point of the DFS, it can be guaranteed that all cycles have been added to unfiltered list at least once.
In the next step all duplicates from the unfiltered list will be removed.
After that cycles containing other cycles will be removed.
A cycle contains another cycle if they share a path of 3 vertices.
What you end up with is a filtered list of minimal cycles.
These cycles represent the polygons needed for the next step.
In the next step the polygons will be projected onto the heightmap.
Then the polygon is selected that contains the desired mountain peak location.
By raising only the heightmap vertices that lie in the selected polygon and repeating the same technique over and over using different sets of polygons it is possible to generate procedural mountains at arbitrary locations.
How can you determine if a point is in a polygon or not?
Actually there is a pretty intuitive solution to this problem.
By drawing a line from the point in any direction and counting the intersections it is guaranteed that the point is outside if there is an even number of intersections and inside if there is an odd number of intersections.
Here is a small demonstration of what would happen if you would perform 100 iterations of this algorithm with the desired mountain peak at the center of the heightmap.
Unfortunately, graph-based RMP is not very efficient.
This is due to minimal cycle search not scaling well with increasing number of polygons needed for RMP.
All it comes down to is generating a set of random polygons.
For this purpose, a graph is not absolutely necessary.
Voronoi diagrams seem to produce a similar pattern, as seen with graph-based RMP.
The question is, how can we cheaply generate these diagrams?
Actually there is a intuitive solution to this problem.
By generating cones at random positions, and looking at the scene from the top using orthogonal projection, a Voronoi diagram can be spotted.
This algorithm takes advantage of OpenGLs depth testing and due to its nature is very fast.
It scales much better than the graph-based alternative.
Here you can see Voronoi diagrams with 10, 100 and 1000 cones / polygons.
The result looks similar to graph-based RMP but there is a dramatic performance difference.
Let's have a look.
I prepared 3 tasks for each algorithm and compared them based on their execution time.
I did 10 experiments per task and aggregated the results.
The goal of task 1 was to generate 7 polygons per iteration for 100 iterations.
Voronoi-based RMP was about 1 second faster than graph-based RMP.
The goal of task 2 was to generate 11 polygons per iteration for 100 iterations.
In this case Voronoi-based RMP took about 2 seconds while graph-based RMP took about 46.
Graph-based RMP performs even worse on task 3.
It took about one and a half hour on average.
Voronoi on the other hand took again about 2 seconds for the same task.
Since Voronoi was performing so well, I was interested in the limits of Voronoi.
Therefore I conducted another experiment where I generated way more polygons than before.
In fact I prepared a graph that should depict this experiment.
On the horizontal axis you can the see the number of polygons to be generated for 100 iterations and on the vertical axis the time that passed by during generation.
Closely to 1000 polygons the generation duration just starts rising.
This is truly impressive and led to the abandonment of graph-based RMP, since Voronoi was much faster and guaranteed real-time capability.
Ok, now that we got the basics covered, let's move on to the more advanced stuff.
I will introduce you to two different categories.
The geology-based terrain generation algorithms and the geology-based terrain degradation algorithms.
Let's start with the geology-based terrain generation algorithms.
For geology-based terrain generation it was necessary to somehow model the processes that were originally involved in the creation of mountains on earth.
So how did mountains come into existance on earth?
Earth consists of two essential layers:
The solid lithosphere and the liquid asthenosphere.
The solid lithosphere is divided into distinct plates which float on top of the liquid asthenosphere.
There are several ways these plates can interact with each other:
They can grind past each other, causing major earth quakes but no terrain generation or degradation.
They can slide apart from each other, causing vulcanic activity at the seperation border, which can ultimately lead to terrain generation.
They can slide toward each other and ultimately collide, causing them to pile up in this process. The highest mountains on earth, including the mount everest, were created by two lithospheric plates colliding with each other.
As you can imagine, tectonic plate movement, is a complicated process and therefore it was necessary to reduce the simulation to two dimensions.
I created a simplified tectonic plate simulation using freely moving polygons in a two dimensional space and kept track of polygon colissions. The produced collision data will be recorded and later used to generate mountain ranges along the collision borders.
A physics engine called Box2D was used to detect polygon collisions.
Unfortunately splitting of polygons was not supported by Box2D and had to be implemented by hand.
I will now show you the basic concept behind splitting convex polygons.
Now lets say you have an arbitrary convex polygon.
The only thing you know about it are the vertices it consists of.
Then you want to split it at an arbitrary location.
One possible way is to shoot a ray towards the polygon and remember the entry and exit point of the ray.
After that we iterate through all polygon vertices and decide whether they are on one or the other side of the ray.
This can be done using the vector product similarly as we have seen before at the fault algorithm.
The entry and exit point have to be added to both parts of the newly split polygon.
Let's see how this technique is used for splitting tectonic plates during initialization.
Box2D can be used to log collision events every time two polygons collide.
A collision event contains data about which polygon vertices were involved in each collision.
Since we have more than enough data, we filter for collisions in which exactly two polygon vertices are involved.
This filtered collision data can be used to generate mountain ranges using an advanced version of Voronoi-based RMP algorithm.
Before I explain the advanced version of Voronoi-based RMP algorithm, let's do a short recap of how the basic Voronoi-based RMP algorithm works.
So the way the basic RMP algorithm works, is that it takes an arbitrary point on the heightmap, generates a Voronoi diagram, projects the diagram onto the heightmap, remembers the color that was projected on the selected point, checks which heightmap vertices have the same color projected onto them and raises them in height.
Now the advanced version works in a similar fashion, but it is capable to generate mountain ranges between two arbitrary points.
Instead of a single polygon, an array of polygons will be raised in height.
The array contains all polygons that are intersected by a straight line between the two points defining the mountain range.
Let me demonstrate a single iteration of this algorithm to generate a mountain range between the top left and bottom right corner.
The outlined polygons are the polygons we are interested in.
The first step is to capture all polygon colors along the mountain range line.
For this purpose Bresenham line algorithm can be used.
Once all colors have been captured we can continue with the next step.
I will now zoom a little bit closer.
Let's assume the Voronoi diagram has been projected onto the heightmap and these are single heightmap tiles.
To determine wheter a heightmap vertex has to be raised in height, we just have to check if any of the captured colors have been projected onto it.
Now since we just do color checks, a concern might be that polygons could have duplicate colors, causing the wrong vertices to be raised in height.
Luckily, polygons having the same color are very unlikely.
In fact there are 2 to the power of 24 colors available.
Here you can see the result of applying 50 iterations of this algorithm using the left top as start point and the bottom right as end point.
If you look very closely you can see that the mountain range peak looks like it is cut off.
This is due to the constant height incrementation each iteration.
When we switch to a random height incrementation per iteration, the mountain peaks again are definite.
The last type of advanced terrain algorithms, I want to talk about, are geology-based terrain degradation algorithms.
Terrain degradation is mainly caused by terrain erosion.
There are several types of terrain erosion, there is erosion due to temperature variations, erosion due to water, wind erosion and glazial erosion.
In my bachelor project I studied and implemented two of these geological phenomena: thermal and hydraulic erosion.
Thermal erosion is the kind of erosion that takes place due to temperature variations.
Due to the lack of sun at night the terrain cools down and heats up again once the sun rises.
These temperature variations as well as gravity as contributing factor can cause structural weakening of the terrain and ultimately wear it down as time passes by.
Hydraulic erosion on the other hand is a medium-based type of erosion.
Water is capable to dissolve terrain sediment and transport it to another location.
As the water evaporates, the dissolved sediment will be deposited again.
Modelling these types of geological phenomena can quite difficult.
Fortunately there are models that are able to mimic similar behavior with a reasonable amount of detail.
Let me begin with the cellular automata-based thermal and hydraulic erosion algorithm.
So what is a cellular automaton in general?
It consists of a grid of cells.
It follows simple rules that tell it how cells interact with each other.
There are so called neighborhoods.
Each cell is part of a neighborhood and can only interact with other cells in its neighborhood.
A cell can be part of more than one neighborhood.
Cellular automata are step-based.

Ok we will now do a small example with a 1D cellular automata.
So each automaton has an initial state.
In this case there are 9 cells, all initialized with 0 but the middle one which is initialized with one.
The neighborhood definition is simply a group of three connected cells.
The rules can be seen down here.
So a neighborhood consisting of 3 zeros will result in a zero after performing a step.

Lets play it through.
The edge cases need a special treatment.
They can either be set to a constant value, for example one or zero, or they can be solved by wrapping around the neighborhood.
In this case i decided to use the wrapping around technique.
So, a neighborhood of 3 zeros results in a zero.
The next step is similar.
So is the step after that.
Now we have 001 which results in 1.
Next we have 010 which results in 0.
100 results in 1.
And now we have all zeros again.
The edge case again will be solved by wrapping around.
Ok so this was a single step.
If you repeat the same procedure several times, you can see that a pattern emerges.
If you fill out all cells containing ones you can see it even better.
Ok now this isn't too impressive is it?
If we use a higher resolution though, the result is quite astonishing.
Despite the simple ruleset, cellular automata can produce incredibly complex results.
They can also be used to model geological phenomena like thermal and hydraulic erosion.
Here we have a 2D cellular automata.
The difference is that the neighborhood not only affects cells of a single row of cells but also cells of other rows.
The neighborhood seen here is a so-called rotated Von-Neumann neighborhood and has been used for this purpose.
The rules for thermal erosion are a little bit different.
The three bars on the left side represent heightmap vertices before applying the first step.
The bars on the right side display the change after performing the first step.
Cells are altered in height, if the height difference between two cells next to each other is greater than the threshold T.
To obey the rules it is necessary to distribute the currently observed cell's height to the other cells in its neighborhood.
Here you can see a heightmap generated with fault algorithm.
Fault algorithm is known to produce very rough and spiky terrain.
Thermal erosion can be used to get rid of these spikes and make it look more realistic.
Cellular automata-based hydraulic erosion algorithm is very similar to thermal erosion but yet different.
Here we do not only have a single heightmap to manage but several ones, 3 in fact.
One for the normal heightmap, one for the waterlevel and one for the dissolved sediment.
The rules consider that fluids like water tend to level out and contain logic for water distribution.
a is the sum of heightmap height and watermap height.
a with a line on the top is the average sum of heightmap height and watermap height in the observed neighborhood.
If a minus a with line on the top is smaller than w, then the excess water a - a with line on the top has to be distributed to the other cells in the neighborhood.
If a minus a with line on the top is greater w, all the water w from the current cell has to be distributed to the other cells in the neighborhood.
The result of the cellular automata-based hydraulic erosion algorithm is kinda subtle.
I mean the effects are visible, but I expected more.
The main problem is that with cellular automata-based hydraulic erosion it is very difficult to realistically model fluid velocity.
That's why I started over and looked into a different way to model hydraulic erosion, namely smoothed particle hydrodynamics.
So what is smoothed particle hydrodynamics?
It is a technique to discretize fluid motion using particles based on Navier-Stokes equations.
I decided not to write everything by myself and rely on an existing codebase instead.
I stumbled upon an interesting project on github written by a guy called Saeed Mahani.
He wrote a basic SPH simulation that simulates how particles behave in a box.
The only thing missing was support for heightmaps.
Ultimately we want the particles to be affected by the terrain and not just bounce around in a box.
So how can we add this feature?
Actually there are two intuitive solutions to this problem.
We can either use static, non-moving particles, distributed all over the heightmap surface.
Or we can apply an impulse on collision with the heightmap surface.
I actually tried the first one but there were several problems.
First of all, it was possible for particles to fall through small wholes inbetween the static particles.
Secondly, the sheer amount of additional particles slowed down the whole simulation to a degree that it was no longer realtime capable.
This was not an option and I decided to use the other solution.
So how do impulses work?
Most importantly, they need a point they are applied to and a direction.
The point they need to be applied to is the collision point of the particle with the heightmap surface.
To determine if a particle collided with the heightmap we have to compare the elevation of the heightmap at the particle coordinates p_x and p_z. If the heightmap elevation at position p_x and p_z is equal or larger than p_y then the particle collided with the heightmap.
Now there is a slight problem.
Since the heightmap elevation is only known at the vertex locations we have to figure out a way to estimate the height inbetween the triangle surface.
I decided to use a ray-triangle intersection algorithm to estimate the height at the collision point.
A cheaper solution would be to interpolate the height between the vertices in concern.
This approach would work fine for large heightmap resolutions but the interpolation error becomes quite noticeable at low heightmap resolutions.
Especially at steep slopes this leads to too much deviation as can be seen here.
Using Möller-Trumbore triangle ray intersection this leads to much better results and the performance is still acceptable.
Now that we have a pretty exact impulse origin location, we still need to determine the impulse direction.
For this we can simply use the normal vectors of the heightmap.
The normal vectors are available per vertex and simply have to be interpolated per triangle.
Interpolation of unit vectors can simply be done by adding them up and then bringing them back to unit length.
Now that we have both impulse origin and impulse direction we can make the particles affected by the heightmap.
What's the purpose of bouncing particles?
How does this help us to model hydraulic erosion?
The idea is that each particle can absorb sediment when it touches the heightmap surface.
Each particle has a sediment capacity.
Each particle has a predefined lifetime and once it is over it evaporates and leaves behind the sediment it carried.
Using these simple rules it is possible to create a very realistic model of hydraulic erosion as can be seen here.
At the end I want to talk a little bit about 2 particular visualization techniques that were used in this bachelor thesis.
Let's start with texture splatting.
Texture splatting describes the technique used to transition from one texture to another.
Here you can see the first row of pixels from the previous image.
For simplicity we pretend we have a really bad resolution.
Here you can see we simply have colors that somehow have to be merged together in a smooth fashion.
For this purpose linear interpolation can be used.
Each color consists of 3 color channels (red, green and blue) and is basicly a 3 dimensional vector.
To interpolate between 2 colors the following formula can be used.
c(t) = a + (b - a) * t
t a real number between 0 and 1
If t is 0.0, c is the same color as defined by a.
If t is 1.0, c is the same color as defined by b.
If t is 0.25, c is 25% of a and 75% of b.
Ok now that we know how to interpolate between 2 colors how can we use this technique in practice?
The heightmap you see down below has been textured automatically.
The textures that have been used are snow, rock, grass and sand.
I used 2 different metrics to achieve this result: height and slope thresholds.
Let's talk about height thresholds.
These thresholds are the bare minimum that any application working with heightmaps should provide.
You basically have a seperate threshold for each texture that you want to use.
At the threshold borders you blend together the textures to avoid visual discontinuities.
A more advanced texturing technique is to use slope thresholds in addition to height thresholds.
This allows to remove visible texture transition lines between texture thresholds.
To determine the slope at an arbitrary location on the heightmap you can simply use the y-coordinate of the normalized normal vectors.
In real life grass does not grow on steep slopes and therefore is overriden by the rock texture.
Same goes with snow.
Snow does not rest on steep slopes and as well gets overriden by the rock texture.
By considering slope thresholds as well, some special cases occured that required a recursive invocation of the linear interpolation formula.
In addition to texture transitions it is helpful to use seamless textures, in order to avoid visual borders between textures.
Seamless textures can simply be tiled without producing visual inconsistencies.
Besides texture splatting I also implemented a so called Bloom shader.
The idea is to reproduce image imperfections caused by real-world cameras.
If you take a picture of a brightly lit area with an ordinary camera it often appears to be glowing.
This same effect can be achieved by using a combination of two different filters:
A bright-bass filter and a Gauss-blur filter.
The purpose of the bright-pass filter is to let bright colors pass in an unimpeded manner and to filter out dark colors.
The Gauss-blur filter on the other hand is used to unsharpen the image.
I prepared a simple example of a 3 by 3 Gauss-blur filter.
To get the filtered color of a pixel we take the weighted average of the pixel color and all colors in its neighborhood.
Of course this technique is not very efficient.
You have to imagine that we have a HD resolution of 1280 by 720 pixels and render 60 frames per second.
This would already be about 50 million operations per second.
As a performance optimization, it has been suggested to use two seperate Gauss-blur filters:
A horizontal Gauss-blur filter and a vertical Gauss-blur filter.
If you apply them in a sequential fashion it has about the same effect but is much more efficient than the other version.
The bright-pass is then the addition of the colors from the input image and the image that results if you apply the bright-pass filter followed by the horizontal and vertical gauss filter to the input image.
Here you can see the image before.
And here you can see the image after.
Now that we're close to the end of the presentation, I'd like to mention what technologies have been used to implement this bachelor project.
First of all, this is an OpenGL-based project.
GLWF has been used as an OpenGL context provider.
GLWF is a multi-platform library and offers native support for UNIX-based platforms as well as Windows.
I wrote this project in C++ using the Xcode integrated development environment.
I never worked with Xcode before, but I was surprised how easy it was to set it up with all the dependencies that I needed.
This image is a screenshot of Saeed Mahani's smoothed particle hydrodynamics simulation that I extended and used to simulate for my hydraulic erosion algorithm.
Box2D is the physics library that I have extended by adding support for splitting of arbitrary convex polygons and used to simulate tectonic plate movement.
stblib is an amazing C library written by Sean Thomas Barret.
It offers several general purpose functions such as a function that converts a character array filled with rgb pixel values to a compressed .png image file.
ffmpeg was used to export an array of frames to a compressed .mpg video file.
I hope you enjoyed my presentation and if you have any questions feel free to ask.
Thanks for your attention.
